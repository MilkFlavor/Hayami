#!/bin/bash

# shellcheck source=/dev/null
source ./src/help # Show help message
source ./src/args # Get sub from user (get_subreddit)
source ./src/version # Show version

# args
while [ $# -gt 0 ]; do
	case $1 in
		-h|--help)
			help_info
			exit 0
			;;
		-l|--limit)
			shift
			LIMIT=$1
			case $LIMIT in
				''|*[!0-9]*)
					echo 'limit is NaN'
					exit 1
			esac
			shift
			;;
		-v|--version)
			version
			exit 0
			;;
		*)
			subreddit=$1
			shift
			;;
	esac
done

# Default config directory
configdir="${XDG_CONFIG_HOME:-$HOME/.config}/redyt"

# Create .config/redyt if it does not exist to prevent
# the program from not functioning properly
[ ! -d "$configdir" ] && echo "Directory $configdir does not exist, creating..." && mkdir -p "$configdir"

# If subreddit.txt does not exist, create it to prevent
# the program from not functioning properly
[ ! -f "$configdir/subreddit.txt" ] && echo "File $configdir/subreddit.txt does not exist, creating..." && touch "$configdir/subreddit.txt"

# If no argument is passed
get_subreddit

# Default directory used to store the feed file and fetched images
cachedir="./images"

# If cachedir does not exist, create it
if [ ! -d "$cachedir" ]; then
	echo "Directory $cachedir does not exist, creating..."
	mkdir -p "$cachedir"
fi

# Set user agent
useragent=$(shuf -e "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4482.0 Safari/537.36 Edg/92.0.874.0" "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1 Safari/605.1.15" -n 1)

# Download the subreddit feed, containing only the
# first 100 entries (limit), and store it inside
# cachedir/tmp.json
curl -H "User-agent: '$useragent'" "https://www.reddit.com/r/$subreddit/hot.json?limit=${LIMIT:-100}" > "$cachedir/tmp.json"

# Create a list of images
imgs=$(jq '.' < "$cachedir/tmp.json" | grep url_overridden_by_dest | grep -Eo "http(s|)://.*(jpg|png)\b" | sort -u)

# Download images to $cachedir
for img in $imgs; do
	if [ ! -e "$cachedir/${img##*/}" ]; then
		wget -U "$useragent" -P "$cachedir" "$img"
	fi
done

# Once finished, remove all of the cached images
#[[ "$KEEP" != "1" ]] && rm "${cachedir:?}"/*