#!/bin/bash

scrap() {
    # Set user agent
    useragent=$(shuf -e "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4482.0 Safari/537.36 Edg/92.0.874.0" "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1 Safari/605.1.15" -n 1)

    # Download the subreddit feed, containing only the
    # first 100 entries (limit), and store it inside
    # cachedir/tmp.json
    curl -H "User-agent: '$useragent'" "https://www.reddit.com/r/$1/hot.json?limit=${LIMIT:-100}" > "$2/tmp.json"

    # Create a list of images
    imgs=$(jq '.' < "$2/tmp.json" | grep url_overridden_by_dest | grep -Eo "http(s|)://.*(jpg|png)\b" | sort -u)

    # Download images to $cachedir
    for img in $imgs; do
        if [ ! -e "$2/${img##*/}" ]; then
            wget -U "$useragent" -P "$2" "$img"
        fi
    done
}